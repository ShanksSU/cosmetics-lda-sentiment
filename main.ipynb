{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd5ef86",
   "metadata": {},
   "source": [
    "# 連接資料庫並取得評論資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, bindparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954331ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = 'root'\n",
    "db_password = '123456'\n",
    "db_host = '100.77.42.49'\n",
    "db_port = '3306'\n",
    "db_name = 'lda_temp'\n",
    "\n",
    "# 建立連線\n",
    "connection_string = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}?charset=utf8mb4\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT 1\"))\n",
    "        print(\"連線成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"連線失敗：{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_file_path = 'sql_query/5b.sql'\n",
    "sql_file_name = os.path.splitext(os.path.basename(sql_file_path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 讀取 SQL 檔案\n",
    "    if not os.path.exists(sql_file_path):\n",
    "        raise FileNotFoundError(f\"找不到檔案：{sql_file_path}\")\n",
    "\n",
    "    with open(sql_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        sql_query = file.read()\n",
    "\n",
    "    # 執行查詢並放入 dataFrame\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(text(sql_query), conn)\n",
    "\n",
    "    print(f\"讀取成功！共取得 {len(df)} 筆資料\")\n",
    "    # print(df.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"發生錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817ec61",
   "metadata": {},
   "source": [
    "# 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = f\"result_{sql_file_name}\"\n",
    "STOP_WORDS_PATH = r\"stop_dic/stopwords.txt\"\n",
    "CUSTOM_DICT_PATH = r'stop_dic/dict.txt'\n",
    "POS_MAPPING_CSV_PATH = r\"stop_dic/CKIP_Tag_Mapping_Table.csv\"\n",
    "DATA_PATH = r'data'\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"{torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CKIPTokenizer:\n",
    "    \"\"\"\n",
    "    CKIPTokenizer 類別用於整合中研院 CKIP Transformers 斷詞工具\n",
    "    \"\"\"\n",
    "    def __init__(self, stopwords_path, pos_mapping_csv_path, custom_dict_path=None, device=0):\n",
    "        \"\"\"\n",
    "        初始化斷詞器與 CKIP 模型。\n",
    "        :param stopwords_path: 停用詞檔案路徑 (.txt)\n",
    "        :param pos_mapping_csv_path: 詞性對照表路徑 (.csv)，用於決定保留哪些詞性\n",
    "        :param custom_dict_path: 自定義詞典路徑 (.txt)\n",
    "        :param device: 執行設備 (0 為 GPU, -1 為 CPU)\n",
    "        \"\"\"\n",
    "        self.stopwords = self.load_stopwords(stopwords_path)\n",
    "        self.custom_dict = self.load_custom_dict(custom_dict_path) if custom_dict_path else {}\n",
    "        self.pos_tags_to_keep = self.load_pos_tags_to_keep(pos_mapping_csv_path)\n",
    "        \n",
    "        # 初始化 CKIP 三大核心驅動器\n",
    "        self.ws_driver  = CkipWordSegmenter (model=\"albert-base\", device=device) # 斷詞\n",
    "        self.pos_driver = CkipPosTagger     (model=\"albert-base\", device=device) # 詞性標記\n",
    "        self.ner_driver = CkipNerChunker    (model=\"albert-base\", device=device) # 實體辨識\n",
    "        \n",
    "    def load_stopwords(self, filepath):\n",
    "        \"\"\"載入停用詞檔案，回傳一個 Set 以提升搜尋效率\"\"\"\n",
    "        with open(filepath, encoding='utf-8-sig') as f:\n",
    "            return set(line.strip() for line in f)\n",
    "\n",
    "    def load_custom_dict(self, filepath):\n",
    "        \"\"\"載入自定義詞典，格式預期為：詞彙 [權重]\"\"\"\n",
    "        custom_terms = {}\n",
    "        with open(filepath, encoding='utf-8-sig') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 1:\n",
    "                    word = parts[0]\n",
    "                    # 若無指定權重，預設為 1.0\n",
    "                    weight = float(parts[1]) if len(parts) > 1 else 1.0\n",
    "                    custom_terms[word] = weight\n",
    "        return custom_terms\n",
    "\n",
    "    def load_pos_tags_to_keep(self, csv_path):\n",
    "        \"\"\"從 CSV 載入需要保留的詞性標記\"\"\"\n",
    "        pos_tags = []\n",
    "        with open(csv_path, newline='', encoding='utf-8-sig') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                # 檢查 CSV 中 ' need(0/1) ' 欄位是否為 '1'\n",
    "                if row['need(0/1)'].strip() == '1':\n",
    "                    tags = row['簡化標記']\n",
    "                    pos_tags.append(tags)\n",
    "        return pos_tags\n",
    "\n",
    "    def tokenize(self, text, return_pos=False):\n",
    "        \"\"\"\n",
    "        對單一字串進行斷詞\n",
    "        :param text: 輸入純文字\n",
    "        :param return_pos: 是否回傳 (詞, 詞性) 元組清單\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # 清洗文字：僅保留中文 (Regex: \\u4e00-\\u9fff)\n",
    "        text = re.sub(r'[^\\u4e00-\\u9fff]', '', text)\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            # 執行斷詞與詞性標記\n",
    "            word_sentence_list = self.ws_driver([text], show_progress=False)\n",
    "            pos_sentence_list = self.pos_driver(word_sentence_list, show_progress=False)\n",
    "\n",
    "            words = word_sentence_list[0]\n",
    "            pos_tags = pos_sentence_list[0]\n",
    "\n",
    "            filtered_words = []\n",
    "            filtered_word_pos = []\n",
    "            \n",
    "            # 過濾邏輯：1.不在停用詞內 2.字數>=2 3.屬於保留詞性\n",
    "            for word, pos in zip(words, pos_tags):\n",
    "                if (word not in self.stopwords) and (len(word) >= 2) and (pos in self.pos_tags_to_keep):\n",
    "                    filtered_words.append(f\"{word}\")\n",
    "                    filtered_word_pos.append((word, pos))\n",
    "\n",
    "            if return_pos:\n",
    "                return filtered_word_pos\n",
    "            else:\n",
    "                return \" \".join(filtered_words) # 回傳以空格分隔的字串\n",
    "        except Exception as e:\n",
    "            print(f\"CKIP processing error: {e}\")\n",
    "            return \"\" if not return_pos else []\n",
    "        \n",
    "    def batch_tokenize(self, text_list, batch_size=64):\n",
    "        \"\"\"\n",
    "        批次處理多筆文字\n",
    "        :param text_list: 字串列表\n",
    "        :param batch_size: 批次處理大小\n",
    "        \"\"\"\n",
    "        clean_texts = []\n",
    "        valid_indices = [] # 用於記錄非空值的位置，以便最後還原清單長度\n",
    "        \n",
    "        print(\"正在清理文字...\")\n",
    "        for idx, text in enumerate(text_list):\n",
    "            if isinstance(text, str):\n",
    "                # 這裡保留了中英數字 ( \\w )，與單筆 tokenize 的邏輯略有不同\n",
    "                text = re.sub(r'[^\\u4e00-\\u9fff\\w]', '', text) \n",
    "                if text.strip():\n",
    "                    clean_texts.append(text)\n",
    "                    valid_indices.append(idx)\n",
    "        \n",
    "        if not clean_texts:\n",
    "            return [[] for _ in range(len(text_list))]\n",
    "\n",
    "        # 批次執行模型運算\n",
    "        print(f\"執行 CKIP 模型 (Batch Size: {batch_size})...\")\n",
    "        ws_list = self.ws_driver(clean_texts, batch_size=batch_size, show_progress=True)\n",
    "        pos_list = self.pos_driver(ws_list, batch_size=batch_size, show_progress=True)\n",
    "\n",
    "        print(\"過濾停用詞與篩選詞性...\")\n",
    "        final_tokens_list = []\n",
    "        \n",
    "        for words, pos_tags in zip(ws_list, pos_list):\n",
    "            filtered_words = []\n",
    "            for word, pos in zip(words, pos_tags):\n",
    "                # 條件篩選：非停用詞、長度 > 1、指定詞性\n",
    "                if (word not in self.stopwords) and (len(word) > 1) and (pos in self.pos_tags_to_keep):\n",
    "                    filtered_words.append(word)\n",
    "            \n",
    "            final_tokens_list.append(filtered_words)\n",
    "\n",
    "        # 依照原始 list 的索引將結果放回對應位置，其餘填空 list\n",
    "        result = [[] for _ in range(len(text_list))]\n",
    "        for i, tokens in zip(valid_indices, final_tokens_list):\n",
    "            result[i] = tokens\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b93cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CKIPTokenizer(\n",
    "    stopwords_path=STOP_WORDS_PATH, \n",
    "    pos_mapping_csv_path=POS_MAPPING_CSV_PATH, \n",
    "    custom_dict_path=CUSTOM_DICT_PATH, \n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"處理 DataFrame...\")\n",
    "df['tokens'] = tokenizer.batch_tokenize(df['評論'].tolist(), batch_size=256)\n",
    "print(df[['評論', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df598e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"result_tokens.csv\"\n",
    "save_path = os.path.join(OUTPUT_PATH, file_name)\n",
    "\n",
    "print(f\"\\n準備儲存檔案至: {save_path}\")\n",
    "\n",
    "try:\n",
    "    # 檢查目錄是否存在，不存在就建立\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "        print(f\"已建立資料夾: {OUTPUT_PATH}\")\n",
    "\n",
    "    df[['評論', 'tokens']].to_csv(\n",
    "        save_path, \n",
    "        index=False, \n",
    "        encoding='utf-8-sig'\n",
    "    )\n",
    "    \n",
    "    print(f\"成功保存，檔案大小約: {os.path.getsize(save_path) / 1024:.2f} KB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"保存失敗: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3b49a",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97421456",
   "metadata": {},
   "source": [
    "## 建立詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5162b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "print(corpus.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5176e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000  # 只取前 1000 個最重要的詞\n",
    "\n",
    "# 建立 Vectorizer\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_features=n_features,\n",
    "    max_df=0.95,             # 如果一個詞在 95% 的文章都出現，就刪掉 (太通用)\n",
    "    min_df=5,                # 至少要出現 5 次才算數 (過濾錯字或極罕見詞)\n",
    "    token_pattern=r'(?u)\\b\\w+\\b' # 確保匹配中文\n",
    ")\n",
    "\n",
    "# 轉換為矩陣 (詞袋（Bag-of-Words）)\n",
    "tf = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"矩陣形狀: {tf.shape} (評論數, 特徵詞數)\")\n",
    "feature_names = tf_vectorizer.get_feature_names_out() # 詞彙表\n",
    "print(f\"前 50 個特徵詞: {feature_names[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5532a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算出每個詞在所有文章中的總次數\n",
    "# tf 是一個稀疏矩陣 (sparse matrix)，sum() 出來會是一個矩陣物件\n",
    "# .A1 是 numpy 的屬性，可以快速將矩陣攤平變成一維陣列 (array)\n",
    "total_counts = tf.sum(axis=0).A1\n",
    "\n",
    "# 建立 DataFrame 對照表 (詞, 次數)\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'Term': feature_names,\n",
    "    'Frequency': total_counts\n",
    "})\n",
    "\n",
    "# 依照次數由大到小排序\n",
    "word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)\n",
    "print(\"\\n=== 前 20 個高頻詞 ===\")\n",
    "print(word_freq_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b594033",
   "metadata": {},
   "source": [
    "## 訓練 LDA 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定主題數量\n",
    "n_topics = 10\n",
    "\n",
    "print(f\"開始訓練 LDA 模型 (主題數: {n_topics})...\")\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,                  # 主題數量\n",
    "                                max_iter=100,                           # 疊代次數\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50,\n",
    "                                doc_topic_prior= (1 / n_topics),        # 預設為 1 / n_topics，主題分布的稀疏程度（alpha）\n",
    "                                topic_word_prior= (1 / n_features),     # 預設為 1 / n_features，詞分布的稀疏程度（Beta）\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "print(\"模型訓練完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db449528",
   "metadata": {},
   "source": [
    "## 查看主題關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14532567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    print(\"\\n=== 各主題關鍵字列表 ===\")\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"主題 #{topic_idx + 1}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "# 顯示每個主題前 15 個關鍵字\n",
    "print_top_words(lda, feature_names, n_top_words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf83f4a",
   "metadata": {},
   "source": [
    "## 輸出每個文章對應的主題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "topics=lda.transform(tf)\n",
    "topic = []\n",
    "for t in topics:\n",
    "    topic.append(list(t).index(np.max(t)))\n",
    "df['topic'] = topic\n",
    "df.to_csv(f\"{OUTPUT_PATH}/data_topic.csv\",index = False, encoding='utf-8-sig')\n",
    "# topics[0] #0 1 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61101fb8",
   "metadata": {},
   "source": [
    "## web可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "print(pyLDAvis.__version__)\n",
    "import pyLDAvis.lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 Jupyter Notebook 中，直接顯示\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.lda_model.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存成 HTML 檔案\n",
    "html_path = os.path.join(OUTPUT_PATH, 'result_lda.html')\n",
    "pyLDAvis.save_html(vis, html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d9af7",
   "metadata": {},
   "source": [
    "# 文本相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import numpy as np\n",
    "import tqdm as notebook_tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_PATH = r\"stop_dic/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 專利摘要文本1，申請號：CN202480029985.2\n",
    "patent1 = \"本公開涉及通信方法、通信設備、通信系統及存儲介質。通信方法包括：AF向第一網元發送第一資訊，所述第一資訊用於指示取消A‑IoT設備的週期性操作。本公開通過AF向第一網元發送第一資訊，以指示取消A‑IoT設備的週期性操作，從而有效減少A‑IoT設備的能量消耗。\"\n",
    "\n",
    "# 專利摘要文本2，申請號：CN202480000774.6\n",
    "patent2 = \"本公開涉及通信方法、通信設備、通信系統及存儲介質。該方法包括：第一實體接收網路功能服務消費方發送的第一請求，所述第一請求包括網路功能服務提供方相關的第一資訊，所述第一資訊是基於所述網路功能服務提供方的第二資訊進行處理得到的資訊，所述網路功能服務消費方位於第一網路，所述網路功能服務提供方位於第二網路；所述第一實體發送第二請求。通過本公開實施例，可以提高通信安全性。\"\n",
    "\n",
    "# 專利摘要文本3，申請號：CN202410796819.7\n",
    "patent3 = \"本公開涉及一種螢幕的顯示方法、裝置、設備、存儲介質和程式產品，涉及螢幕顯示技術領域，方法包括：獲取終端生成的待顯示資料包，待顯示資料包包括目標幀對應的幀同步信號、目標幀中每個圖元行對應的行同步信號以及目標顯示資料；確定終端的螢幕對應的面板時序；根據面板時序，對行同步信號進行補償，得到目標行同步信號；根據幀同步信號、目標行同步信號和目標顯示資料，生成目標顯示信號，並在終端的螢幕上進行顯示。這樣，根據終端螢幕對應的面板時序對待顯示資料包中的行同步信號進行補償，從而避免了螢幕出現花屏、橫屏、亮帶等等異常情況，保證了螢幕的顯示效果和穩定性，提升了用戶體驗。\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b03e5",
   "metadata": {},
   "source": [
    "## 1. 傳統方法_TF-IDF\n",
    "Ref: https://doi.org/10.1111/jofi.12885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9736527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加載停用詞\n",
    "def load_stopwords(filepath):\n",
    "    with open(filepath, encoding='utf-8-sig') as f:\n",
    "        return set(line.strip() for line in f)\n",
    "\n",
    "STOPWORDS = load_stopwords(STOP_WORDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. jieba 分詞\n",
    "def segment_sentence(sentence: str) -> list:\n",
    "    words = jieba.cut(sentence, cut_all=False)\n",
    "    return [w.strip() for w in words if w.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44dc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 去除停用詞\n",
    "def remove_stopwords(words: list) -> list:\n",
    "    return [w for w in words if w not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 去除標點符號\n",
    "def remove_punctuation(words: list) -> str:\n",
    "    joined = \" \".join(words)\n",
    "    cleaned = re.sub(r\"[^0-9A-Za-z\\u4e00-\\u9fa5\\s]\", \" \", joined)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 總流程\n",
    "def preprocess_document(text: str) -> list:\n",
    "    # Step1 分詞\n",
    "    words = segment_sentence(text)\n",
    "\n",
    "    # Step2 去停用詞\n",
    "    words = remove_stopwords(words)\n",
    "\n",
    "    # Step3 去標點符號\n",
    "    cleaned_list = remove_punctuation(words)\n",
    "\n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7510817",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = preprocess_document(patent1)\n",
    "text2 = preprocess_document(patent2)\n",
    "text3 = preprocess_document(patent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40454333",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94870830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_tfidf_cosine_similarity(text1_words, text2_words):\n",
    "    \"\"\"\n",
    "    使用 TF-IDF 方法計算兩個已完成分詞與去停用詞文本的餘弦相似度 (Cosine Similarity)。\n",
    "    \n",
    "    參數:\n",
    "        text1_words: 第一個文本的分詞結果（字串形式，詞之間以空格分隔；或詞彙列表）\n",
    "        text2_words: 第二個文本的分詞結果（字串形式，詞之間以空格分隔；或詞彙列表）\n",
    "    \n",
    "    回傳:\n",
    "        float: 餘弦相似度值（範圍：0-1，1 表示方向完全相同/高度相似）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 確保輸入為字串格式（若輸入為列表 list，則將其合併為以空格分隔的字串）\n",
    "    if isinstance(text1_words, list):\n",
    "        text1_words = ' '.join(text1_words)\n",
    "    if isinstance(text2_words, list):\n",
    "        text2_words = ' '.join(text2_words)\n",
    "    \n",
    "    # 初始化 TF-IDF 向量化工具\n",
    "    # 這會將文本轉換為詞頻-逆文件頻率矩陣\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    try:\n",
    "        # 擬合數據並轉換文本為 TF-IDF 矩陣\n",
    "        # 矩陣包含兩列向量，分別代表 text1 與 text2\n",
    "        tfidf_matrix = vectorizer.fit_transform([text1_words, text2_words])\n",
    "        \n",
    "        # 計算兩個向量之間的餘弦相似度\n",
    "        # cosine_similarity 預期輸入為矩陣，故使用切片 [0:1] 與 [1:2]\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        \n",
    "        return similarity\n",
    "    except ValueError:\n",
    "        # 若文本為空或不包含任何可辨識的詞彙，TfidfVectorizer 可能會拋出錯誤\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity12 = calculate_tfidf_cosine_similarity(text1, text2)\n",
    "similarity13 = calculate_tfidf_cosine_similarity(text1, text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"專利1與專利2的相似度: {similarity12:.4f}\\n專利1與專利3的相似度: {similarity13:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9c58a",
   "metadata": {},
   "source": [
    "## 2. 現代方法_大語言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6dc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"./bge-base-zh-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_model.encode([patent1, patent2, patent3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos12 = embedding_model.similarity(embedding[0], embedding[1])\n",
    "cos13 = embedding_model.similarity(embedding[0], embedding[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d892ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"專利1與專利2的相似度: {cos12.item():.4f}\\n專利1與專利3的相似度: {cos13.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_222",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
